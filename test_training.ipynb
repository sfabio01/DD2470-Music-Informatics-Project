{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from model import Song2Vec\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sabbi\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\nn\\modules\\transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    }
   ],
   "source": [
    "cnn_input_channels = 1  # Assuming the input is a 1D array\n",
    "cnn_output_channels = 512  # Number of output channels after CNN\n",
    "transformer_input_dim = cnn_output_channels  # Should match the output channels from the CNN\n",
    "embed_dim = 16  # Output embedding size for transformer\n",
    "num_heads = 1\n",
    "num_layers = 1\n",
    "ff_dim = 16\n",
    "\n",
    "model = Song2Vec(cnn_input_channels, cnn_output_channels, transformer_input_dim, embed_dim, num_heads, num_layers, ff_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import CNNFeatureExtractor\n",
    "\n",
    "cnn = CNNFeatureExtractor(3, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1024, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn(torch.randn(1, 3, 1024, 2048)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(1, 3, 1024, 2048)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_loss_fn = nn.TripletMarginLoss(margin=1.0, p=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embedding shape:  torch.Size([4, 16])\n",
      "p:  torch.Size([1])\n",
      "torch.Size([1, 3, 1024, 2048])\n"
     ]
    }
   ],
   "source": [
    "anchor, positive, negative = torch.randn(1, 3, 1024, 2048), torch.randn(1, 4, 3, 1024, 2048), torch.randn(1, 4, 3, 1024, 2048)\n",
    "        \n",
    "shape = positive.shape\n",
    "\n",
    "# reshape positive and negative to (batch_size * 20, n_channels, height, width)\n",
    "positive = positive.view(-1, *positive.shape[2:])\n",
    "negative = negative.view(-1, *negative.shape[2:])\n",
    "\n",
    "anchor_embed = model(anchor)\n",
    "with torch.no_grad():\n",
    "    positive_embed = model(positive) # shape (batch_size * 20, embed_dim)\n",
    "    negative_embed = model(negative)\n",
    "\n",
    "print(\"embedding shape: \", positive_embed.shape)\n",
    "\n",
    "# reshape positive_embed and negative_embed to (batch_size, 20, embed_dim)\n",
    "positive_embed = positive_embed.view(shape[0], shape[1], -1)\n",
    "negative_embed = negative_embed.view(shape[0], shape[1], -1)\n",
    "\n",
    "dist_pos = F.cosine_similarity(anchor_embed.unsqueeze(1), positive_embed, dim=-1) # shape (batch_size, 20)\n",
    "dist_neg = F.cosine_similarity(anchor_embed.unsqueeze(1), negative_embed, dim=-1)\n",
    "\n",
    "p = torch.argmin(dist_pos, dim=1) # shape (batch_size,)\n",
    "n = torch.argmax(dist_neg, dim=1)\n",
    "\n",
    "# reshape positive and negative back to (batch_size, 20, n_channels, height, width)\n",
    "positive = positive.view(shape[0], shape[1], *positive.shape[1:])\n",
    "negative = negative.view(shape[0], shape[1], *negative.shape[1:])\n",
    "\n",
    "# recompute positive and negative embeddings with gradients\n",
    "positive_embed = model(positive[torch.arange(shape[0]), p]) # shape (batch_size, embed_dim)\n",
    "negative_embed = model(negative[torch.arange(shape[0]), n])\n",
    "\n",
    "loss = triplet_loss_fn(anchor_embed, positive_embed, negative_embed)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
